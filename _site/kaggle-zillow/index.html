<!DOCTYPE html>
<html>
  <head>
    
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="shortcut icon" href="/static/img/favicon.ico" />
        <title>Kaggle - Zillow - Marco Tompitak</title>
        <meta name="author" content="Marco Tompitak" />
        <meta name="description" content="Kaggle - Zillow" />
        <meta name="keywords" content="Kaggle - Zillow, Marco Tompitak, " />

        <meta content="0" property="fb:app_id">
        <meta content="Marco Tompitak" property="og:site_name">
        
          <meta content="Kaggle - Zillow" property="og:title">
        
        
          <meta content="article" property="og:type">
        
        
          <meta content="My GitHub Page" property="og:description">
        
        
          <meta content="http://localhost:4000/kaggle-zillow/" property="og:url">
        
        
        
          <meta content="http://localhost:4000/static/img/logo-high-resolution.png" property="og:image">
        
        
        
        <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@\#">
        <meta name="twitter:creator" content="@\#">
        
          <meta name="twitter:title" content="Kaggle - Zillow">
        
        
          <meta name="twitter:url" content="http://localhost:4000/kaggle-zillow/">
        
        
          <meta name="twitter:description" content="My GitHub Page">
        
        

      <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml">
      <script type="text/javascript">window.baseurl = 'http://localhost:4000';</script>
      
        <!-- Custom Fonts -->
        <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:300,400,500,700" type="text/css">

        <!-- FontAwesome icons -->
        <link rel="stylesheet" href="https://use.fontawesome.com/74dfc6cf47.css">

        <!-- Core BootStrap CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
        <!-- Material Design CSS -->
        <link rel="stylesheet" href="/static/css/bootstrap-material-design.min.css">

        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/static/css/syntax.css">

        <!-- Custom CSS -->        
        <link rel="stylesheet" href="/static/css/thickbox.css">
        <link rel="stylesheet" href="/static/css/main.css">
        <link rel="stylesheet" href="/static/css/projects.css">

        <script type="text/javascript">
          //loadingImage is relative to project dir
          var tb_pathToImage = "/static/img/loadingAnimation.gif";
        </script>

  </head>

  <body class="home overflow-hidden">
    <div class="header-panel shadow-z-2">
      <div class="container">
        <div class="row">
          <div class="col-md-3 col-sm-4 col-xs-12">
            <div class="row-picture">
              <img id="about" class="logo-img" src="/static/img/avatar.svg" height="75px" width="75px">
            </div>
            <div class="row-details">
              <h4 class="list-group-item-heading">Marco Tompitak</h4>
              <p class="list-group-item-text">Biophysicist</p>
              <div class="social-icons">
	
        <a class="icon" target="_blank" href="https://github.com/marcotompitak"><i class="fa fa-github"></i></a>
    
        <a class="icon" target="_blank" href="https://www.linkedin.com/in/marcotompitak/"><i class="fa fa-linkedin"></i></a>
    
        <a class="icon" target="_blank" href="https://stackexchange.com/users/2808839/marco-tompitak"><i class="fa fa-stack-exchange"></i></a>
    
        <a class="icon" target="_blank" href="http://www.rapiddiffusion.com/"><i class="fa fa-pencil-square-o"></i></a>
    
</div>

            </div>
            <div class="navbar-header pull-right">
              <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <i class="fa fa-2x fa-bars"></i>
              </button>
            </div>
          </div>
          <div class="col-md-9 col-sm-8 col-xs-12">
          <div class="row">
            <h2 class="blog-title-pro ">Kaggle - Zillow</h2>
            
            </div>
            <p class="info">
              
              
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="container main outer">
      <div class="row">
        <div class="col-md-3 col-xs-12">
              <nav class="menu">
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
    <ul class="list-separator nav navbar-nav well well-primary page">

	
	
	
	
	
	
    
	
	<li class="col-lg-12 col-md-12 col-sm-4 col-xs-12  kaggle-zillow"><a href="/" target="_self"><i class="fa fa-home"></i> Home</a></li>

	
	
	
	
	
	
    
	
	<li class="col-lg-12 col-md-12 col-sm-4 col-xs-12  kaggle-zillow"><a href="/about/" target="_self"><i class="fa fa-user"></i> About</a></li>

	
	
	
	
	
	
    
	
	<li class="col-lg-12 col-md-12 col-sm-4 col-xs-12  kaggle-zillow"><a href="/projects" target="_self"><i class="fa fa-desktop"></i> Projects</a></li>

	
	
	
	
	
	
    
	
	<li class="col-lg-12 col-md-12 col-sm-4 col-xs-12  kaggle-zillow"><a href="/publications" target="_self"><i class="fa fa-file-text-o"></i> Publications</a></li>

	
	
	
	
	
	
    
	
	<li class="col-lg-12 col-md-12 col-sm-4 col-xs-12  kaggle-zillow"><a href="https://github.com/marcotompitak" target="_self"><i class="fa fa-github"></i> Github</a></li>

</ul>

    </div>
    </nav>

        </div>
        <div class="col-md-9 col-xs-12 full">
          <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<div class="well" style="text-align:justify">
    
    <div>
    	<h2 id="kaggle-competition-zillow-home-valuation-prediction">Kaggle Competition: Zillow Home Valuation Prediction</h2>

<p><em>(This is a static version of an iPython notebook. The actual notebook and further code can be found on <a href="https://github.com/marcotompitak/kaggle-code/tree/master/Zillow">GitHub</a>.)</em></p>

<p>I’m going to use XGBoost to fit a boosted tree ensemble to the data from the <a href="https://www.kaggle.com/c/zillow-prize-1">Zillow Kaggle Competition</a>. The quantity to predict is the size of the error that Zillow’s own algorithm has made in predicting housing prices, based on a large number of variables describing the house. Since we’re trying to capture the errors in model that’s already being used, it will probably be difficult to make any headway, but that’s why it’s a competition.</p>

<p>First we will load the required libraries, load in the data, and preprocess the data.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="kn">as</span> <span class="nn">xgb</span>
<span class="kn">import</span> <span class="nn">gc</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Competition training data</span>
<span class="n">known</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'input/train_2016_v2.csv'</span><span class="p">)</span>

<span class="c"># Full set of properties</span>
<span class="n">population</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'input/properties_2016.csv'</span><span class="p">)</span>

<span class="c"># Properties for which to predict in order to </span>
<span class="n">unknown</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'input/sample_submission.csv'</span><span class="p">)</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Some variables are categorical, and we will split them up using OneHot encoding</span>
<span class="n">categorical_cols</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'airconditioningtypeid'</span><span class="p">,</span>
    <span class="s">'architecturalstyletypeid'</span><span class="p">,</span>
    <span class="s">'buildingclasstypeid'</span><span class="p">,</span>
    <span class="s">'decktypeid'</span><span class="p">,</span>
    <span class="s">'fips'</span><span class="p">,</span>
    <span class="s">'fireplaceflag'</span><span class="p">,</span>
    <span class="s">'hashottuborspa'</span><span class="p">,</span>
    <span class="s">'heatingorsystemtypeid'</span><span class="p">,</span>
    <span class="s">'storytypeid'</span><span class="p">,</span>
    <span class="s">'typeconstructiontypeid'</span><span class="p">,</span>
    <span class="s">'taxdelinquencyflag'</span><span class="p">,</span>
<span class="p">]</span>

<span class="c"># Some variables take on very many categorical values. </span>
<span class="c"># For the sake of this exercise, we'll drop them.</span>
<span class="n">cols_to_drop</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'rawcensustractandblock'</span><span class="p">,</span>
    <span class="s">'censustractandblock'</span><span class="p">,</span>
    <span class="s">'propertycountylandusecode'</span><span class="p">,</span>
    <span class="s">'propertylandusetypeid'</span><span class="p">,</span>
    <span class="s">'propertyzoningdesc'</span><span class="p">,</span>
    <span class="s">'regionidcounty'</span><span class="p">,</span>
    <span class="s">'regionidcity'</span><span class="p">,</span>
    <span class="s">'regionidzip'</span><span class="p">,</span>
    <span class="s">'regionidneighborhood'</span><span class="p">,</span>
    <span class="s">'taxdelinquencyyear'</span>
<span class="p">]</span>

<span class="c"># Creating our full dataset</span>
<span class="n">df_known</span> <span class="o">=</span> <span class="n">known</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s">'left'</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">'parcelid'</span><span class="p">)</span>

<span class="c"># Dropping selected columns</span>
<span class="n">df_known</span> <span class="o">=</span> <span class="n">df_known</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">cols_to_drop</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># Re-encoding categorical variables</span>
<span class="n">df_known_cat</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df_known</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">categorical_cols</span><span class="p">)</span>

<span class="c"># Transforming the transaction date into an </span>
<span class="c"># ordinal variable: number of days since 01-01-2015</span>
<span class="n">df_known_cat</span><span class="p">[</span><span class="s">'transactiondate_ordinal'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df_known_cat</span><span class="p">[</span><span class="s">'transactiondate'</span><span class="p">],</span><span class="n">infer_datetime_format</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">-</span> <span class="n">datetime</span><span class="o">.</span><span class="n">date</span><span class="p">(</span><span class="mi">2015</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_known_cat</span><span class="p">[</span><span class="s">'transactiondate_ordinal'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_known_cat</span><span class="p">[</span><span class="s">'transactiondate_ordinal'</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">days</span>
<span class="n">df_known_cat</span> <span class="o">=</span> <span class="n">df_known_cat</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'transactiondate'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># Creating our variables and targets</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_known_cat</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">"logerror"</span><span class="p">,</span> <span class="s">"parcelid"</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_known_cat</span><span class="p">[</span><span class="s">"logerror"</span><span class="p">]</span>

<span class="c"># Randomly splitting into a training and a validation set</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre>
</div>

<p>We are now ready for our first attempt at training.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Setting up initial parameters for our model, </span>
<span class="c"># with arbitrary guesses for the numerical parameters.</span>
<span class="n">xgb_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"objective"</span><span class="p">:</span> <span class="s">"reg:linear"</span><span class="p">,</span> 
    <span class="s">"max_depth"</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> 
    <span class="s">"silent"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> 
    <span class="s">'eval_metric'</span><span class="p">:</span> <span class="s">'mae'</span><span class="p">,</span> 
    <span class="s">'eta'</span><span class="p">:</span> <span class="mf">0.01</span>
<span class="p">}</span>

<span class="c"># Maximum number of trees we will collect</span>
<span class="n">num_rounds</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c"># Transforming our data into XGBoost's internal DMatrix structure</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dvalid</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c"># Training</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">xgb_params</span><span class="p">,</span>                           <span class="c"># Training parameters</span>
    <span class="n">dtrain</span><span class="p">,</span>                               <span class="c"># Data</span>
    <span class="n">num_rounds</span><span class="p">,</span>                           <span class="c"># Max number of trees</span>
    <span class="p">[(</span><span class="n">dtrain</span><span class="p">,</span><span class="s">'train'</span><span class="p">),(</span><span class="n">dvalid</span><span class="p">,</span><span class="s">'test'</span><span class="p">)],</span>   <span class="c"># Validation set</span>
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>            <span class="c"># Stop early if no improvement for 100 rounds</span>
    <span class="n">verbose_eval</span> <span class="o">=</span> <span class="bp">False</span>
<span class="p">)</span>

<span class="c"># Best score obtained</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">best_score</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>0.068397
</code></pre>
</div>

<h2 id="improving-the-model">Improving the model</h2>

<p>The score above is already pretty decent. At the time of writing, the scores in the Kaggle competition range from around 0.068 to around 0.064. With the model above we are already at the low end. The small range of scores compared to this base score is an indication of how hard this particular problem is.</p>

<p>Let us try to improve upon our score. For this exercise, we will not optimize all the hyperparameters of the model, but we will optimize a few. We will use a grid search, using cross validation to assess the performance of the model. In order to be able to run this (computationally expensive) search, this optimization is actually done in two separate Python scripts: <code class="highlighter-rouge">CV_eta.py</code> and <code class="highlighter-rouge">CV_maxdepth_minchildweight.py</code>.</p>

<p>As indicated by the names, the first script optimizes <code class="highlighter-rouge">eta</code>, or the learning rate, and the second simulateneously optimizeds <code class="highlighter-rouge">max_depth</code> (the maximum tree depth) and <code class="highlighter-rouge">min_child_weight</code>, which in this case places a lower bound on the number of data points on each side of a split in the tree.</p>

<p>Looking for the optimal learning rate produces the following plot.</p>

<p><img src="/pages/Zillow_files/eta.png" alt="" /></p>

<p>In order to accommodate slower learning, we increased the number of trees for this optimization. We see that, in that case, slower learning performs better, pushing the optimal <code class="highlighter-rouge">eta</code> down to 0.005. However, the validation error remains at 0.68, so as long as the use the right number of trees, our original rate was fine.</p>

<p>Considering <code class="highlighter-rouge">max_depth</code> and <code class="highlighter-rouge">min_child_weight</code>, we find the following results.</p>

<p><img src="/pages/Zillow_files/cv2.png" alt="" /></p>

<p>We see that there is no benefit to increasing <code class="highlighter-rouge">min_child_weight</code> to more than 1. The corresponding curve for <code class="highlighter-rouge">max_depth</code> is fairly flat: the model is not very sensitive to this value, so we don’t expect a big performance boost from tuning it. To avoid overfitting, we will go to smaller depths.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># We slightly speed up learning for convenience, </span>
<span class="c"># and decrease the tree depth</span>
<span class="n">xgb_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"objective"</span><span class="p">:</span> <span class="s">"reg:linear"</span><span class="p">,</span> 
    <span class="s">"max_depth"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> 
    <span class="s">"silent"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> 
    <span class="s">'eval_metric'</span><span class="p">:</span> <span class="s">'mae'</span><span class="p">,</span> 
    <span class="s">'eta'</span><span class="p">:</span> <span class="mf">0.02</span>
<span class="p">}</span>

<span class="c"># Maximum number of trees we will collect</span>
<span class="n">num_rounds</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c"># Transforming our data into XGBoost's internal DMatrix structure</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dvalid</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c"># Training</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">xgb_params</span><span class="p">,</span>                           <span class="c"># Training parameters</span>
    <span class="n">dtrain</span><span class="p">,</span>                               <span class="c"># Data</span>
    <span class="n">num_rounds</span><span class="p">,</span>                           <span class="c"># Max number of trees</span>
    <span class="p">[(</span><span class="n">dtrain</span><span class="p">,</span><span class="s">'train'</span><span class="p">),(</span><span class="n">dvalid</span><span class="p">,</span><span class="s">'test'</span><span class="p">)],</span>   <span class="c"># Validation set</span>
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>             <span class="c"># Stop early if no improvement for 50 rounds</span>
    <span class="n">verbose_eval</span> <span class="o">=</span> <span class="bp">False</span>
<span class="p">)</span>

<span class="c"># Best score obtained</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">best_score</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>0.068047
</code></pre>
</div>

<p>We could tune further hyperparameters, but the limiting factor here is likely the data itself, judging from the range of scores on the leaderboard, and we should not expect much of a performance boost from tuning. We will therefore leave the parameters for what they are for now, and move on to a different potential improvement.</p>

<h2 id="cutting-out-outliers">Cutting out outliers</h2>

<p>The dataset contains a large number of outliers, including some very large/small ones:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">known</span><span class="p">[</span><span class="s">'logerror'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'logerror'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="/pages/Zillow_files/Zillow_9_0.png" alt="png" /></p>

<p>Apparently there are houses in the dataset for which Zillow’s algorithm made large errors. Normally one might think that the outliers are the most interesting to look at, because that’s where there is a lot of room for improvement. However, for our current purposes this may not be the case, because</p>

<p>1) the error may be large simply because the price of the house is high;</p>

<p>2) for high-value houses, not much data is available, so we may not be able to learn much;</p>

<p>3) the market for high-value houses is small, so that prices are dependent on the tastes and funds of available buyers, leading to unpredictable variance.</p>

<p>Of course the above may or may not be the real underlying reason for the outliers, but it suggests that it may be worth trying to remove the outliers from the training set, because they may not contain much generalizable information.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Quick function to find outliers. A data point in a series is considered</span>
<span class="c"># an outlier if it is more than the inter-quartile range (IQR), multiplied</span>
<span class="c"># by some factor, away from the first or third quartile.</span>
<span class="k">def</span> <span class="nf">getIQROutlierBool</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">factor</span><span class="p">):</span>
    <span class="n">q1</span> <span class="o">=</span> <span class="n">series</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span> <span class="c"># First quartile</span>
    <span class="n">q3</span> <span class="o">=</span> <span class="n">series</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span> <span class="c"># Third quartile</span>
    
    <span class="c"># Generate a boolean series indicating whether or not the data point is</span>
    <span class="c"># within the specified range</span>
    <span class="n">iqr</span> <span class="o">=</span> <span class="p">(</span><span class="n">series</span> <span class="o">&gt;</span> <span class="n">q1</span><span class="o">-</span><span class="n">factor</span><span class="o">*</span><span class="p">(</span><span class="n">q3</span><span class="o">-</span><span class="n">q1</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">series</span> <span class="o">&lt;</span> <span class="n">q3</span><span class="o">+</span><span class="n">factor</span><span class="o">*</span><span class="p">(</span><span class="n">q3</span><span class="o">-</span><span class="n">q1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">iqr</span>

<span class="c"># Apply the filter to our logerror values</span>
<span class="n">outlier_filter</span> <span class="o">=</span> <span class="n">getIQROutlierBool</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>

<span class="c"># Cut out the outliers from the training set</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">outlier_filter</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">outlier_filter</span><span class="p">]</span>
</code></pre>
</div>

<p>Above, we have filtered out the outliers from the training set (not from the validation set) by removing all data points that are too far away from the first and third quartiles. If a value is smaller than the first quartile, or larger than the third quartile, by at least 1.5 times the interquartile range (distance between first and third quartiles), we discard it. The factor 1.5 is the standard value, but some manual checks indicate that the result is not strongly dependent on the choice.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># We keep the parameters</span>
<span class="n">xgb_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"objective"</span><span class="p">:</span> <span class="s">"reg:linear"</span><span class="p">,</span> 
    <span class="s">"max_depth"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> 
    <span class="s">"silent"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> 
    <span class="s">'eval_metric'</span><span class="p">:</span> <span class="s">'mae'</span><span class="p">,</span> 
    <span class="s">'eta'</span><span class="p">:</span> <span class="mf">0.02</span>
<span class="p">}</span>

<span class="c"># Maximum number of trees we will collect</span>
<span class="n">num_rounds</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="c"># Our X_train and y_train have now been cleaned of outliers</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dvalid</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c"># Training</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">xgb_params</span><span class="p">,</span>                           <span class="c"># Training parameters</span>
    <span class="n">dtrain</span><span class="p">,</span>                               <span class="c"># Data</span>
    <span class="n">num_rounds</span><span class="p">,</span>                           <span class="c"># Max number of trees</span>
    <span class="p">[(</span><span class="n">dtrain</span><span class="p">,</span><span class="s">'train'</span><span class="p">),(</span><span class="n">dvalid</span><span class="p">,</span><span class="s">'test'</span><span class="p">)],</span>   <span class="c"># Validation set</span>
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>             <span class="c"># Stop early if no improvement for 50 rounds</span>
    <span class="n">verbose_eval</span> <span class="o">=</span> <span class="bp">False</span>
<span class="p">)</span>

<span class="c"># Best score obtained</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">best_score</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">best_iteration</span><span class="p">)</span>

<span class="c"># Save the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s">'xgb_outliersremoved.model'</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>0.067132
1136
</code></pre>
</div>

<p>We see that removing the outliers from the training set has improved the score a little bit. We are not yet near the top of the leaderboard (0.064) but we’ve nudged away from the bottom.</p>

<p>(Note that here we are just validating on the test set. The model is saved above, to be used in <code class="highlighter-rouge">predict.py</code> to generate a submission file for the competition.)</p>

<h2 id="conclusions">Conclusions</h2>

<p>The Zillow dataset is difficult. We have a predictive model that works, in a sense, but not very well, considering the following plot.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Cut out the outliers from the test set for plotting</span>
<span class="n">outlier_filter</span> <span class="o">=</span> <span class="n">getIQROutlierBool</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">outlier_filter</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">outlier_filter</span><span class="p">]</span>
<span class="n">dtest</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c"># Predict with our model</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dtest</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Real values'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Predicted values'</span><span class="p">)</span>

<span class="c"># Add a trend line</span>
<span class="n">regression</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">])</span>
<span class="n">y_vals</span> <span class="o">=</span> <span class="n">regression</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x_vals</span> <span class="o">+</span> <span class="n">regression</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'green'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="/pages/Zillow_files/Zillow_15_0.png" alt="png" /></p>

<p>The figure above plots the <code class="highlighter-rouge">logerror</code> values we predict for the validation set versus the true values, as well as a trend line. We see that we have some correlation, but the slope is certainly not close to 45 degrees, and the spread of the points is very large. Our score of 0.67 does not indicate a very strong grasp on the problem, and likely neither do the top scores on the Kaggle leaderboard of around 0.64.</p>

<p>Tuning the hyperparameters apparently helps little. Solving this problem requires something new altogether; either a different kind of model is required, or some novel insight into the data. Cutting out the outliers in the training set helps somewhat, but only so much. It will be interesting to see if anyone can find a way forward.</p>

    </div>
</div>

          <div class="row">
            <div class="col-md-12 col-xs-12 footer">
              <footer>
  © 2017 Marco Tompitak - Powered by Jekyll.
</footer>

            </div>
          </div>
        </div> <!-- end /.col-md-9 -->
      </div> <!-- end /.row -->
    </div>

    
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<script src="/static/js/thickbox-compressed.js"></script>
<script src="/static/js/material.min.js"></script>
<script src="/static/js/main.js"></script>
<script src="/static/js/projects.js"></script>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-47199852-2', 'auto');
  ga('send', 'pageview');

</script>
  </body>
</html>
